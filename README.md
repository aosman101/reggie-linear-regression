# reggie-linear-regression

An educational walk-through of simple linear regression. This project re-creates â€œReggieâ€™s Linear Regressionâ€ from scratch (slope/intercept model, absolute/squared error, gradient descent to fit `m` and `b`), then validates the result against `scikit-learn`â€™s implementation and visualises the outcome.

> If youâ€™re new to README formatting on GitHub, this file uses standard Markdown, so it renders nicely in the repo UI.

## ğŸš€ Highlights

âœ… From-scratch Ordinary Least Squares (OLS) model: y = m * x + b.
âš™ï¸ Implementation of absolute and squared error functions.
ğŸ” Gradient descent training loop for learning slope/intercept.
ğŸ§  Comparison with sklearn.linear_model.LinearRegression.
ğŸ§¾ Clean, commented notebook for step-by-step learning.
ğŸ’¡ Lightweight â€” no external datasets required (uses generated data).
ğŸ“Š New: Companion notebook Linear-Regression-Plots.ipynb showcasing diverse Matplotlib plots for documentation or reporting.

> Reference: `scikit-learn`â€™s `Linear Regression` minimises residual sum of squares for OLS. See the official docs for API details.  

## ğŸ“ Repository structure

â”œâ”€ Reggie_Linear_Regression.ipynb. # main notebook (from-scratch + sklearn comparison).

â”œâ”€ Linear-Regression-Plots.ipynb # 5 reusable Matplotlib plots (line, scatter, bar, box, heatmap).

â”œâ”€ data/ # (optional) sample CSVs if added later.

â”œâ”€ imgs/ # (optional) saved output plots.

â””â”€ README.md

